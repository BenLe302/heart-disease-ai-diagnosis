{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü´Ä Analyse Compl√®te du Diagnostic de Maladie Cardiaque\n",
    "\n",
    "## Table des Mati√®res\n",
    "\n",
    "1. [Introduction et Objectifs](#1-introduction-et-objectifs)\n",
    "2. [Exploration des Donn√©es](#2-exploration-des-donn√©es)\n",
    "3. [Pr√©paration des Donn√©es](#3-pr√©paration-des-donn√©es)\n",
    "4. [Mod√©lisation](#4-mod√©lisation)\n",
    "5. [√âvaluation des Mod√®les](#5-√©valuation-des-mod√®les)\n",
    "6. [Analyse Avanc√©e](#6-analyse-avanc√©e)\n",
    "7. [Optimisation](#7-optimisation)\n",
    "8. [Validation Finale](#8-validation-finale)\n",
    "9. [Interpr√©tation Clinique](#9-interpr√©tation-clinique)\n",
    "10. [Conclusions](#10-conclusions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction et Objectifs\n",
    "\n",
    "### Contexte\n",
    "Les maladies cardiovasculaires sont la premi√®re cause de mortalit√© dans le monde. Ce projet vise √† d√©velopper un syst√®me de diagnostic automatis√© utilisant des techniques d'apprentissage automatique pour pr√©dire la pr√©sence de maladie cardiaque.\n",
    "\n",
    "### Objectifs\n",
    "- Analyser les facteurs de risque de maladie cardiaque\n",
    "- D√©velopper des mod√®les pr√©dictifs performants\n",
    "- Fournir des outils d'aide √† la d√©cision clinique\n",
    "- Cr√©er une interface utilisateur intuitive\n",
    "\n",
    "### M√©thodologie\n",
    "1. Exploration et nettoyage des donn√©es\n",
    "2. Ing√©nierie des caract√©ristiques\n",
    "3. Entra√Ænement de multiples mod√®les ML\n",
    "4. √âvaluation et comparaison des performances\n",
    "5. Optimisation des hyperparam√®tres\n",
    "6. Validation finale et d√©ploiement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des biblioth√®ques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration des graphiques\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Affichage de toutes les colonnes\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print('‚úÖ Biblioth√®ques import√©es avec succ√®s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploration des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des donn√©es\n",
    "data = pd.read_csv('../data/raw/heart_disease_uci.csv')\n",
    "\n",
    "print(f'üìä Dimensions du dataset: {data.shape}')\n",
    "print(f'üìà Nombre d\'√©chantillons: {len(data)}')\n",
    "print(f'üìã Nombre de variables: {len(data.columns)}')\n",
    "\n",
    "# Aper√ßu des donn√©es\n",
    "print('\\nüîç Aper√ßu des premi√®res lignes:')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informations sur les types de donn√©es\n",
    "print('üìã Informations sur le dataset:')\n",
    "data.info()\n",
    "\n",
    "print('\\nüìä Statistiques descriptives:')\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rification des valeurs manquantes\n",
    "missing_values = data.isnull().sum()\n",
    "missing_percent = (missing_values / len(data)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Valeurs manquantes': missing_values,\n",
    "    'Pourcentage': missing_percent\n",
    "})\n",
    "\n",
    "print('üîç Analyse des valeurs manquantes:')\n",
    "print(missing_df[missing_df['Valeurs manquantes'] > 0])\n",
    "\n",
    "if missing_df['Valeurs manquantes'].sum() == 0:\n",
    "    print('‚úÖ Aucune valeur manquante d√©tect√©e!')\n",
    "else:\n",
    "    print(f'‚ö†Ô∏è {missing_df[\"Valeurs manquantes\"].sum()} valeurs manquantes au total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution de la variable cible\n",
    "target_counts = data['target'].value_counts()\n",
    "target_percent = data['target'].value_counts(normalize=True) * 100\n",
    "\n",
    "print('üéØ Distribution de la variable cible:')\n",
    "print(f'Pas de maladie cardiaque (0): {target_counts[0]} ({target_percent[0]:.1f}%)')\n",
    "print(f'Maladie cardiaque (1): {target_counts[1]} ({target_percent[1]:.1f}%)')\n",
    "\n",
    "# Visualisation\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Graphique en barres\n",
    "target_counts.plot(kind='bar', ax=ax1, color=['lightcoral', 'lightblue'])\n",
    "ax1.set_title('Distribution de la Variable Cible')\n",
    "ax1.set_xlabel('Classe')\n",
    "ax1.set_ylabel('Nombre d\'√©chantillons')\n",
    "ax1.set_xticklabels(['Pas de maladie', 'Maladie cardiaque'], rotation=0)\n",
    "\n",
    "# Graphique en secteurs\n",
    "ax2.pie(target_counts.values, labels=['Pas de maladie', 'Maladie cardiaque'], \n",
    "        autopct='%1.1f%%', colors=['lightcoral', 'lightblue'])\n",
    "ax2.set_title('R√©partition des Classes')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pr√©paration des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# S√©paration des features et de la target\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "print(f'üìä Features shape: {X.shape}')\n",
    "print(f'üéØ Target shape: {y.shape}')\n",
    "\n",
    "# Identification des variables num√©riques et cat√©gorielles\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f'üî¢ Variables num√©riques ({len(numeric_features)}): {numeric_features}')\n",
    "print(f'üìù Variables cat√©gorielles ({len(categorical_features)}): {categorical_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodage des variables cat√©gorielles\n",
    "X_encoded = X.copy()\n",
    "\n",
    "# Encodage manuel pour maintenir la coh√©rence\n",
    "if 'sex' in X_encoded.columns:\n",
    "    X_encoded['sex'] = X_encoded['sex'].map({'M': 1, 'F': 0})\n",
    "\n",
    "if 'cp' in X_encoded.columns:\n",
    "    cp_mapping = {'typical angina': 0, 'atypical angina': 1, 'non-anginal pain': 2, 'asymptomatic': 3}\n",
    "    X_encoded['cp'] = X_encoded['cp'].map(cp_mapping)\n",
    "\n",
    "if 'fbs' in X_encoded.columns:\n",
    "    X_encoded['fbs'] = X_encoded['fbs'].astype(int)\n",
    "\n",
    "if 'restecg' in X_encoded.columns:\n",
    "    restecg_mapping = {'normal': 0, 'st-t abnormality': 1, 'lv hypertrophy': 2}\n",
    "    X_encoded['restecg'] = X_encoded['restecg'].map(restecg_mapping)\n",
    "\n",
    "if 'exang' in X_encoded.columns:\n",
    "    X_encoded['exang'] = X_encoded['exang'].astype(int)\n",
    "\n",
    "if 'slope' in X_encoded.columns:\n",
    "    slope_mapping = {'upsloping': 0, 'flat': 1, 'downsloping': 2}\n",
    "    X_encoded['slope'] = X_encoded['slope'].map(slope_mapping)\n",
    "\n",
    "if 'thal' in X_encoded.columns:\n",
    "    thal_mapping = {'normal': 3, 'fixed defect': 6, 'reversable defect': 7}\n",
    "    X_encoded['thal'] = X_encoded['thal'].map(thal_mapping)\n",
    "\n",
    "print('‚úÖ Encodage des variables cat√©gorielles termin√©')\n",
    "print(f'üìä Shape apr√®s encodage: {X_encoded.shape}')\n",
    "\n",
    "# V√©rification des valeurs manquantes apr√®s encodage\n",
    "missing_after_encoding = X_encoded.isnull().sum().sum()\n",
    "print(f'üîç Valeurs manquantes apr√®s encodage: {missing_after_encoding}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f'üìä Taille du set d\'entra√Ænement: {X_train.shape}')\n",
    "print(f'üìä Taille du set de test: {X_test.shape}')\n",
    "\n",
    "# V√©rification de la distribution des classes\n",
    "print('\\nüéØ Distribution des classes:')\n",
    "print(f'Train - Classe 0: {(y_train == 0).sum()} ({(y_train == 0).mean():.1%})')\n",
    "print(f'Train - Classe 1: {(y_train == 1).sum()} ({(y_train == 1).mean():.1%})')\n",
    "print(f'Test - Classe 0: {(y_test == 0).sum()} ({(y_test == 0).mean():.1%})')\n",
    "print(f'Test - Classe 1: {(y_test == 1).sum()} ({(y_test == 1).mean():.1%})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardisation des features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Conversion en DataFrame pour garder les noms de colonnes\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print('‚úÖ Standardisation termin√©e')\n",
    "print(f'üìä Moyenne des features (train): {X_train_scaled.mean().mean():.6f}')\n",
    "print(f'üìä √âcart-type des features (train): {X_train_scaled.std().mean():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mod√©lisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des mod√®les\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve\n",
    "import joblib\n",
    "\n",
    "# Initialisation des mod√®les\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "    'CatBoost': CatBoostClassifier(random_state=42, verbose=False)\n",
    "}\n",
    "\n",
    "print('ü§ñ Mod√®les initialis√©s:')\n",
    "for name in models.keys():\n",
    "    print(f'  - {name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Ænement et √©valuation des mod√®les\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "print('üöÄ D√©but de l\'entra√Ænement des mod√®les...\\n')\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f'‚è≥ Entra√Ænement de {name}...')\n",
    "    \n",
    "    # Entra√Ænement\n",
    "    if name in ['Logistic Regression', 'SVM']:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calcul des m√©triques\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'AUC': auc\n",
    "    }\n",
    "    \n",
    "    trained_models[name] = model\n",
    "    \n",
    "    print(f'‚úÖ {name} - Accuracy: {accuracy:.3f}, AUC: {auc:.3f}\\n')\n",
    "\n",
    "print('üéâ Entra√Ænement termin√©!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. √âvaluation des Mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau de comparaison des performances\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df.round(3)\n",
    "\n",
    "print('üìä Comparaison des performances des mod√®les:')\n",
    "print(results_df)\n",
    "\n",
    "# Identification du meilleur mod√®le\n",
    "best_model_name = results_df['AUC'].idxmax()\n",
    "best_auc = results_df.loc[best_model_name, 'AUC']\n",
    "\n",
    "print(f'\\nüèÜ Meilleur mod√®le: {best_model_name} (AUC: {best_auc:.3f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des performances\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i//2, i%2]\n",
    "    values = results_df[metric].values\n",
    "    models_names = results_df.index\n",
    "    \n",
    "    bars = ax.bar(models_names, values, color=plt.cm.Set3(np.linspace(0, 1, len(models_names))))\n",
    "    ax.set_title(f'{metric} par Mod√®le', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # Ajout des valeurs sur les barres\n",
    "    for bar, value in zip(bars, values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbes ROC\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "colors = plt.cm.Set1(np.linspace(0, 1, len(models)))\n",
    "\n",
    "for i, (name, model) in enumerate(trained_models.items()):\n",
    "    if name in ['Logistic Regression', 'SVM']:\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    plt.plot(fpr, tpr, color=colors[i], lw=2, \n",
    "             label=f'{name} (AUC = {auc_score:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Ligne de base (AUC = 0.500)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taux de Faux Positifs', fontsize=12)\n",
    "plt.ylabel('Taux de Vrais Positifs', fontsize=12)\n",
    "plt.title('Courbes ROC - Comparaison des Mod√®les', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyse Avanc√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse de l'importance des features pour les mod√®les bas√©s sur les arbres\n",
    "tree_models = ['Random Forest', 'Gradient Boosting', 'XGBoost', 'CatBoost']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, model_name in enumerate(tree_models):\n",
    "    if model_name in trained_models:\n",
    "        model = trained_models[model_name]\n",
    "        \n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "            feature_names = X_train.columns\n",
    "            \n",
    "            # Tri par importance d√©croissante\n",
    "            indices = np.argsort(importances)[::-1][:10]  # Top 10\n",
    "            \n",
    "            axes[i].bar(range(len(indices)), importances[indices])\n",
    "            axes[i].set_title(f'Importance des Features - {model_name}', fontweight='bold')\n",
    "            axes[i].set_xticks(range(len(indices)))\n",
    "            axes[i].set_xticklabels([feature_names[j] for j in indices], rotation=45, ha='right')\n",
    "            axes[i].set_ylabel('Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Importance moyenne des features\n",
    "if len(tree_models) > 0:\n",
    "    all_importances = []\n",
    "    for model_name in tree_models:\n",
    "        if model_name in trained_models and hasattr(trained_models[model_name], 'feature_importances_'):\n",
    "            all_importances.append(trained_models[model_name].feature_importances_)\n",
    "    \n",
    "    if all_importances:\n",
    "        mean_importances = np.mean(all_importances, axis=0)\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': X_train.columns,\n",
    "            'Importance': mean_importances\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        print('üîç Top 10 des features les plus importantes (moyenne):')\n",
    "        print(feature_importance_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrices de confusion pour les meilleurs mod√®les\n",
    "top_models = results_df.nlargest(3, 'AUC').index.tolist()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, model_name in enumerate(top_models):\n",
    "    model = trained_models[model_name]\n",
    "    \n",
    "    if model_name in ['Logistic Regression', 'SVM']:\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i],\n",
    "                xticklabels=['Pas de maladie', 'Maladie cardiaque'],\n",
    "                yticklabels=['Pas de maladie', 'Maladie cardiaque'])\n",
    "    axes[i].set_title(f'Matrice de Confusion - {model_name}', fontweight='bold')\n",
    "    axes[i].set_xlabel('Pr√©diction')\n",
    "    axes[i].set_ylabel('R√©alit√©')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rapport de classification d√©taill√© pour le meilleur mod√®le\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "if best_model_name in ['Logistic Regression', 'SVM']:\n",
    "    y_pred_best = best_model.predict(X_test_scaled)\n",
    "else:\n",
    "    y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "print(f'üìã Rapport de classification d√©taill√© - {best_model_name}:')\n",
    "print(classification_report(y_test, y_pred_best, \n",
    "                          target_names=['Pas de maladie', 'Maladie cardiaque']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optimisation et Validation Finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimisation des hyperparam√®tres pour les meilleurs mod√®les\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Param√®tres pour l'optimisation\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 6, 10],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    },\n",
    "    'CatBoost': {\n",
    "        'iterations': [100, 200, 300],\n",
    "        'depth': [4, 6, 8],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "}\n",
    "\n",
    "optimized_models = {}\n",
    "\n",
    "print('üîß D√©but de l\'optimisation des hyperparam√®tres...\\n')\n",
    "\n",
    "for model_name in ['Random Forest', 'XGBoost', 'CatBoost']:\n",
    "    if model_name in param_grids:\n",
    "        print(f'‚è≥ Optimisation de {model_name}...')\n",
    "        \n",
    "        base_model = models[model_name]\n",
    "        \n",
    "        # Utilisation de RandomizedSearchCV pour plus d'efficacit√©\n",
    "        grid_search = RandomizedSearchCV(\n",
    "            base_model, \n",
    "            param_grids[model_name],\n",
    "            cv=5,\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=-1,\n",
    "            random_state=42,\n",
    "            n_iter=20  # Nombre d'it√©rations pour RandomizedSearchCV\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        optimized_models[model_name] = grid_search.best_estimator_\n",
    "        \n",
    "        print(f'‚úÖ {model_name} optimis√© - Meilleur score CV: {grid_search.best_score_:.3f}')\n",
    "        print(f'   Meilleurs param√®tres: {grid_search.best_params_}\\n')\n",
    "\n",
    "print('üéâ Optimisation termin√©e!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluation finale sur le set de test\n",
    "final_results = {}\n",
    "\n",
    "print('üìä √âvaluation finale des mod√®les optimis√©s:')\n",
    "print('=' * 50)\n",
    "\n",
    "for name, model in optimized_models.items():\n",
    "    # Pr√©dictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # M√©triques\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    final_results[name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'AUC': auc\n",
    "    }\n",
    "    \n",
    "    print(f'{name}:')\n",
    "    print(f'  Accuracy:  {accuracy:.3f}')\n",
    "    print(f'  Precision: {precision:.3f}')\n",
    "    print(f'  Recall:    {recall:.3f}')\n",
    "    print(f'  F1-Score:  {f1:.3f}')\n",
    "    print(f'  AUC:       {auc:.3f}\\n')\n",
    "\n",
    "# Tableau final\n",
    "final_results_df = pd.DataFrame(final_results).T\n",
    "final_results_df = final_results_df.round(3)\n",
    "\n",
    "print('üìà Tableau de comparaison final:')\n",
    "print(final_results_df)\n",
    "\n",
    "# Meilleur mod√®le final\n",
    "best_final_model = final_results_df['AUC'].idxmax()\n",
    "best_final_auc = final_results_df.loc[best_final_model, 'AUC']\n",
    "\n",
    "print(f'\\nüèÜ Meilleur mod√®le final: {best_final_model} (AUC: {best_final_auc:.3f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion et courbe ROC pour le meilleur mod√®le final\n",
    "best_model_final = optimized_models[best_final_model]\n",
    "y_pred_final = best_model_final.predict(X_test)\n",
    "y_pred_proba_final = best_model_final.predict_proba(X_test)[:, 1]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Matrice de confusion\n",
    "cm_final = confusion_matrix(y_test, y_pred_final)\n",
    "sns.heatmap(cm_final, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
    "            xticklabels=['Pas de maladie', 'Maladie cardiaque'],\n",
    "            yticklabels=['Pas de maladie', 'Maladie cardiaque'])\n",
    "ax1.set_title(f'Matrice de Confusion - {best_final_model}', fontweight='bold')\n",
    "ax1.set_xlabel('Pr√©diction')\n",
    "ax1.set_ylabel('R√©alit√©')\n",
    "\n",
    "# Courbe ROC\n",
    "fpr_final, tpr_final, _ = roc_curve(y_test, y_pred_proba_final)\n",
    "auc_final = roc_auc_score(y_test, y_pred_proba_final)\n",
    "\n",
    "ax2.plot(fpr_final, tpr_final, color='darkorange', lw=2, \n",
    "         label=f'{best_final_model} (AUC = {auc_final:.3f})')\n",
    "ax2.plot([0, 1], [0, 1], 'k--', lw=2, label='Ligne de base (AUC = 0.500)')\n",
    "ax2.set_xlim([0.0, 1.0])\n",
    "ax2.set_ylim([0.0, 1.05])\n",
    "ax2.set_xlabel('Taux de Faux Positifs')\n",
    "ax2.set_ylabel('Taux de Vrais Positifs')\n",
    "ax2.set_title(f'Courbe ROC - {best_final_model}', fontweight='bold')\n",
    "ax2.legend(loc='lower right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sauvegarde des Mod√®les et R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Cr√©ation des dossiers de sauvegarde\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "# Sauvegarde des mod√®les optimis√©s\n",
    "for name, model in optimized_models.items():\n",
    "    model_filename = f'../models/{name.lower().replace(\" \", \"_\")}_model.pkl'\n",
    "    joblib.dump(model, model_filename)\n",
    "    print(f'‚úÖ Mod√®le {name} sauvegard√©: {model_filename}')\n",
    "\n",
    "# Sauvegarde du scaler\n",
    "joblib.dump(scaler, '../models/scaler.pkl')\n",
    "print('‚úÖ Scaler sauvegard√©: ../models/scaler.pkl')\n",
    "\n",
    "# Sauvegarde des donn√©es preprocess√©es\n",
    "X_train.to_csv('../data/processed/X_train.csv', index=False)\n",
    "X_test.to_csv('../data/processed/X_test.csv', index=False)\n",
    "y_train.to_csv('../data/processed/y_train.csv', index=False)\n",
    "y_test.to_csv('../data/processed/y_test.csv', index=False)\n",
    "print('‚úÖ Donn√©es preprocess√©es sauvegard√©es')\n",
    "\n",
    "# Sauvegarde des r√©sultats\n",
    "final_results_df.to_csv('../results/model_performance.csv')\n",
    "print('‚úÖ R√©sultats de performance sauvegard√©s')\n",
    "\n",
    "# Sauvegarde des noms de features\n",
    "feature_names = X_train.columns.tolist()\n",
    "pd.Series(feature_names).to_csv('../data/processed/feature_names.csv', index=False, header=['feature'])\n",
    "print('‚úÖ Noms des features sauvegard√©s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interpr√©tation Clinique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des variables les plus importantes cliniquement\n",
    "if hasattr(best_model_final, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': best_model_final.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print('üîç Variables les plus importantes pour le diagnostic:')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    # Interpr√©tation clinique des top 5 features\n",
    "    clinical_interpretation = {\n",
    "        'cp': 'Type de douleur thoracique - Indicateur cl√© de pathologie cardiaque',\n",
    "        'thalach': 'Fr√©quence cardiaque maximale - Capacit√© cardiovasculaire',\n",
    "        'oldpeak': 'D√©pression ST - Signe d\'isch√©mie myocardique',\n",
    "        'ca': 'Nombre de vaisseaux principaux - √âtendue de la maladie coronaire',\n",
    "        'thal': 'Thalass√©mie - D√©faut de perfusion myocardique',\n",
    "        'exang': 'Angine d\'effort - Sympt√¥me d\'isch√©mie',\n",
    "        'slope': 'Pente du segment ST - R√©ponse √† l\'effort',\n",
    "        'sex': 'Sexe - Facteur de risque d√©mographique',\n",
    "        'age': '√Çge - Facteur de risque principal',\n",
    "        'trestbps': 'Pression art√©rielle - Facteur de risque cardiovasculaire',\n",
    "        'chol': 'Cholest√©rol - Facteur de risque m√©tabolique',\n",
    "        'fbs': 'Glyc√©mie √† jeun - Facteur de risque diab√©tique',\n",
    "        'restecg': 'ECG au repos - Anomalies √©lectriques cardiaques'\n",
    "    }\n",
    "    \n",
    "    for i, (_, row) in enumerate(feature_importance.head(10).iterrows()):\n",
    "        feature = row['Feature']\n",
    "        importance = row['Importance']\n",
    "        interpretation = clinical_interpretation.get(feature, 'Variable clinique')\n",
    "        \n",
    "        print(f'{i+1:2d}. {feature:10s} ({importance:6.1%}) - {interpretation}')\n",
    "    \n",
    "    # Visualisation\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance.head(10)\n",
    "    plt.barh(range(len(top_features)), top_features['Importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Top 10 des Variables les Plus Importantes', fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seuils cliniques et recommandations\n",
    "print('üè• SEUILS CLINIQUES ET RECOMMANDATIONS')\n",
    "print('=' * 50)\n",
    "\n",
    "print('üìä Interpr√©tation des probabilit√©s:')\n",
    "print('  üî¥ RISQUE √âLEV√â (‚â• 70%):')\n",
    "print('     - Consultation cardiologique urgente')\n",
    "print('     - ECG et √©chocardiographie')\n",
    "print('     - Bilan sanguin (troponines, BNP)')\n",
    "print('     - √âviter les efforts intenses')\n",
    "print()\n",
    "print('  üü° RISQUE MOD√âR√â (30-70%):')\n",
    "print('     - Consultation m√©dicale programm√©e')\n",
    "print('     - Test d\'effort si appropri√©')\n",
    "print('     - Contr√¥le des facteurs de risque')\n",
    "print('     - Surveillance r√©guli√®re')\n",
    "print()\n",
    "print('  üü¢ RISQUE FAIBLE (< 30%):')\n",
    "print('     - Maintien d\'un mode de vie sain')\n",
    "print('     - Contr√¥les m√©dicaux annuels')\n",
    "print('     - Pr√©vention primaire')\n",
    "print()\n",
    "\n",
    "print('‚ö†Ô∏è  LIMITATIONS:')\n",
    "print('   - Outil d\'aide √† la d√©cision uniquement')\n",
    "print('   - Ne remplace pas l\'expertise m√©dicale')\n",
    "print('   - Validation sur population sp√©cifique')\n",
    "print('   - Facteurs de risque non exhaustifs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. M√©triques Finales et Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©sum√© des performances du meilleur mod√®le\n",
    "print('üèÜ PERFORMANCE DU MEILLEUR MOD√àLE')\n",
    "print('=' * 40)\n",
    "print(f'Mod√®le: {best_final_model}')\n",
    "print(f'Accuracy:  {final_results_df.loc[best_final_model, \"Accuracy\"]:.1%}')\n",
    "print(f'Precision: {final_results_df.loc[best_final_model, \"Precision\"]:.1%}')\n",
    "print(f'Recall:    {final_results_df.loc[best_final_model, \"Recall\"]:.1%}')\n",
    "print(f'F1-Score:  {final_results_df.loc[best_final_model, \"F1-Score\"]:.1%}')\n",
    "print(f'AUC-ROC:   {final_results_df.loc[best_final_model, \"AUC\"]:.3f}')\n",
    "\n",
    "# Calcul des m√©triques cliniques\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_final).ravel()\n",
    "\n",
    "sensitivity = tp / (tp + fn)  # Recall\n",
    "specificity = tn / (tn + fp)\n",
    "ppv = tp / (tp + fp)  # Precision\n",
    "npv = tn / (tn + fn)\n",
    "\n",
    "print('\\nüìä M√âTRIQUES CLINIQUES:')\n",
    "print(f'Sensibilit√© (Recall):     {sensitivity:.1%}')\n",
    "print(f'Sp√©cificit√©:              {specificity:.1%}')\n",
    "print(f'Valeur Pr√©dictive Pos.:   {ppv:.1%}')\n",
    "print(f'Valeur Pr√©dictive N√©g.:   {npv:.1%}')\n",
    "\n",
    "print('\\nüî¢ MATRICE DE CONFUSION:')\n",
    "print(f'Vrais N√©gatifs:  {tn:3d}   Faux Positifs: {fp:3d}')\n",
    "print(f'Faux N√©gatifs:   {fn:3d}   Vrais Positifs: {tp:3d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perspectives Futures et Am√©liorations\n",
    "\n",
    "### Am√©liorations Techniques\n",
    "1. **Ensemble Methods**: Combinaison de plusieurs mod√®les pour am√©liorer les performances\n",
    "2. **Feature Engineering**: Cr√©ation de nouvelles variables d√©riv√©es\n",
    "3. **Deep Learning**: Exploration des r√©seaux de neurones\n",
    "4. **Cross-validation**: Validation crois√©e plus robuste\n",
    "\n",
    "### Applications Cliniques\n",
    "1. **Int√©gration EMR**: Connexion aux dossiers m√©dicaux √©lectroniques\n",
    "2. **Monitoring Temps R√©el**: Surveillance continue des patients\n",
    "3. **Aide √† la D√©cision**: Support pour les professionnels de sant√©\n",
    "4. **T√©l√©m√©decine**: Diagnostic √† distance\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "Ce projet a d√©montr√© l'efficacit√© des techniques d'apprentissage automatique pour le diagnostic de maladie cardiaque. Les mod√®les d√©velopp√©s ont atteint d'excellentes performances avec une AUC-ROC sup√©rieure √† 85%.\n",
    "\n",
    "### Points Cl√©s:\n",
    "- **Haute Performance**: AUC > 0.85 pour tous les mod√®les optimis√©s\n",
    "- **Interpr√©tabilit√©**: Identification des facteurs de risque principaux\n",
    "- **Applicabilit√© Clinique**: Seuils et recommandations adapt√©s\n",
    "- **Robustesse**: Validation crois√©e et test sur donn√©es ind√©pendantes\n",
    "\n",
    "### Impact Potentiel:\n",
    "- Am√©lioration du diagnostic pr√©coce\n",
    "- R√©duction des co√ªts de sant√©\n",
    "- Support aux professionnels de sant√©\n",
    "- Accessibilit√© du diagnostic dans les zones sous-m√©dicalis√©es\n",
    "\n",
    "---\n",
    "\n",
    "**Avertissement**: Cet outil est destin√© √† des fins de recherche et d'√©ducation. Il ne doit pas remplacer l'avis m√©dical professionnel."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}